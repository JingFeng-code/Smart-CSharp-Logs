---
title: 🚀 902.6-Docker-部署實戰與 AI 整合應用
tags:
  - Docker
  - 部署
  - AI
  - 容器化
  - 開發工具
aliases:
  - Docker 部署實務
  - AI 容器化應用
created: 2025-07-23
updated: 2025-07-23
status: 草稿
summary: 本筆記介紹如何使用 Docker 進行實戰部署，並結合 AI 模型與服務實現容器化應用，提升開發與運維效率。
---

## 🚀 為什麼用 Docker 部署 AI 應用？

- **環境一致性**：避免「在我電腦可用」的問題

- **快速部署與擴展**：容器化讓服務易於複製與擴充

- **資源隔離**：確保 AI 模型運行不受其他服務干擾

- **易於 CI/CD 整合**：搭配流水線自動部署

---

## 🛠 Docker 部署 AI 應用基本流程

1. **建立 Dockerfile**
    
    - 包含基底映像（如 Python、CUDA）
    
    - 安裝模型所需依賴（TensorFlow、PyTorch 等）
    
    - 複製程式碼與模型檔案

2. **建構映像**

```bash
docker build -t ai-app:latest .
```

3. **測試本地容器**

```bash
docker run -p 5000:5000 ai-app:latest
```

1. **部署到生產環境**
    
    - 使用雲端服務（GCP、AWS、Azure）
    
    - 搭配 `docker-compose` 或 Kubernetes

---

## 🤖 AI 應用容器化實例

### 範例：Flask API 封裝 AI 模型

**Dockerfile 範例：**

```dockerfile
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "app.py"]
```

---
## 🔗 整合 GPU 支援

- 使用 NVIDIA Container Toolkit

- 基底映像改用 `nvidia/cuda`

- 執行容器時加上 `--gpus all` 參數

```bash
docker run --gpus all -p 5000:5000 ai-app:latest
```

---
## 🐳 使用 docker-compose 管理多服務

```yaml
version: "3.9"
services:
  ai-api:
    build: .
    ports:
      - "5000:5000"
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
```

---

## 🧪 實戰建議

- 將模型檔與程式碼分開管理，方便更新

- 利用環境變數控制參數設定

- 設置健康檢查與自動重啟策略

- 監控容器資源使用情況

---

## 📚 延伸閱讀

- [[902-Docker-總覽]]
- [[014.19-AI應用-模型部署與容器化 (Docker)]]
- [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)
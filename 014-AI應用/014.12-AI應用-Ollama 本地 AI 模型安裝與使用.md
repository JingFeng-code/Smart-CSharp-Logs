---
title: 🖥️ 014.12-AI應用-Ollama 本地 AI 模型安裝與使用  
tags:
- AI
- Ollama
- 本地模型
- LLM
- 離線推論  
aliases:
- Ollama 本地部署
- Ollama LLM 使用
- 離線大模型  
created: 2025-07-23  
updated: 2025-07-23  
status: 草稿  
summary: 本篇筆記介紹 Ollama 平台的安裝、模型管理與本地化大型語言模型（LLM）推論實作，適合希望在本地或私有環境使用 AI 模型、避免依賴雲端服務的開發者參考。
---


## 什麼是 Ollama？

Ollama 是一套方便在本地端（Mac / Windows / Linux）執行大型語言模型（LLM）的工具平台，支援模型管理、啟動、指令互動與 API 調用。

- 支援多種開源與商業 LLM

- 強調隱私保護與離線推論

- 提供命令列及程式介面

- 支援多模態模型（文字、程式碼等）

---
## 2️⃣ 安裝 Ollama

### 2.1 系統需求

- 作業系統：macOS 12 以上 / Windows 11 / Linux (支援有限)
    
- CPU / GPU 支援依模型大小而異
    
- 建議具備 16GB 以上 RAM 及高速 SSD

### 2.2 安裝步驟（以 macOS 為例）

```bash
# 透過 Homebrew 安裝
brew install ollama/tap/ollama

# 安裝完成後，檢查版本
ollama version
```

Windows 使用者可參考官方安裝檔或 WSL 安裝指南。

---
## 3️⃣ 模型管理

### 3.1 列出可用模型

```bash
ollama list
```

### 3.2 下載並安裝模型

```bash
ollama pull llama2
```

### 3.3 移除模型

```bash
ollama remove llama2
```

---
## 4️⃣ 基本使用

### 4.1 互動式聊天

```bash
ollama run llama2
```

進入互動模式，輸入問題即可獲得回答。

### 4.2 指令式呼叫（Command）

```bash
ollama run llama2 --prompt "請用中文介紹 Ollama 平台"
```

---
## 5️⃣ API 串接與自動化

Ollama 支援用 HTTP API 供應本地模型服務，可用於後端整合。

### 範例：使用 curl 呼叫本地模型

```bash
curl -X POST http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama2",
    "messages": [{"role": "user", "content": "什麼是 Ollama?"}]
  }'
```

---
## 6️⃣ 實務建議

- 選擇適合硬體資源的模型大小（如 7B / 13B）

- 注意本地推論的效能瓶頸與記憶體限制

- 透過 Docker 或容器化管理 Ollama 環境，方便部署

- 定期更新模型與 Ollama 版本以獲得最佳體驗

---

## 🔗 延伸閱讀

- [Ollama 官方網站](https://ollama.com/)
- [Ollama GitHub](https://github.com/ollama/ollama)
- [[014.19-AI應用-模型部署與容器化 (Docker)]]
- [[014.11-AI應用-RAG 與知識檢索強化 LLM 應用]]
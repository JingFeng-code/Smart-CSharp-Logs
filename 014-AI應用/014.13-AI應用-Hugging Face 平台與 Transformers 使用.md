---
title: ğŸ¤— 014.13-AIæ‡‰ç”¨-Hugging Face å¹³å°èˆ‡ Transformers ä½¿ç”¨  
tags:
- AI
- Hugging Face
- Transformers
- é è¨“ç·´æ¨¡å‹
- è‡ªç„¶èªè¨€è™•ç†  
aliases:
- HF å¹³å°ä½¿ç”¨ 
- Transformers æ•™å­¸  
- é è¨“ç·´æ¨¡å‹æ‡‰ç”¨  
created: 2025-07-23  
updated: 2025-07-23  
status: è‰ç¨¿  
summary: æœ¬ç¯‡ç­†è¨˜ä»‹ç´¹ Hugging Face å¹³å°èˆ‡ Transformers å‡½å¼åº«çš„åŸºç¤ä½¿ç”¨ï¼Œæ¶µè“‹æ¨¡å‹æœå°‹ã€ä¸‹è¼‰ã€è¼‰å…¥ã€æ¨è«–èˆ‡å¾®èª¿ï¼Œå¹«åŠ©ä½¿ç”¨è€…å¿«é€Ÿä¸Šæ‰‹å„é¡è‡ªç„¶èªè¨€è™•ç†èˆ‡å¤šæ¨¡æ…‹ä»»å‹™çš„é è¨“ç·´æ¨¡å‹ã€‚
---

## 1ï¸âƒ£ ä»€éº¼æ˜¯ Hugging Faceï¼Ÿ

Hugging Faceï¼ˆHFï¼‰æ˜¯ä¸€å€‹ AI é–‹ç™¼å¹³å°ï¼Œæä¾›ï¼š

- å¤§é‡é è¨“ç·´æ¨¡å‹ï¼ˆNLPã€CVã€å¤šæ¨¡æ…‹ï¼‰

- ä¾¿æ·çš„ Transformers å‡½å¼åº«

- ç¤¾ç¾¤åˆ†äº«æ¨¡å‹èˆ‡è³‡æ–™é›†

- æ–¹ä¾¿çš„æ¨¡å‹æ‰˜ç®¡èˆ‡éƒ¨ç½²æœå‹™ï¼ˆSpacesã€Inference APIï¼‰

---

## 2ï¸âƒ£ Transformers å‡½å¼åº«ç°¡ä»‹

Transformers æ˜¯ Hugging Face å‡ºå“çš„ Python å‡½å¼åº«ï¼Œæ”¯æ´ï¼š

- è¼‰å…¥å„ç¨®é è¨“ç·´æ¨¡å‹ï¼ˆBERTã€GPTã€T5ã€CLIP ç­‰ï¼‰

- å¿«é€Ÿåšæ–‡å­—åˆ†é¡ã€æ‘˜è¦ã€å•ç­”ã€ç”Ÿæˆç­‰ä»»å‹™

- æ”¯æ´ PyTorchã€TensorFlow å…©å¤§æ·±åº¦å­¸ç¿’æ¡†æ¶

- æ”¯æ´è‡ªè¨‚æ¨¡å‹å¾®èª¿èˆ‡æ“´å……

---

## 3ï¸âƒ£ åŸºæœ¬å®‰è£èˆ‡ç’°å¢ƒæº–å‚™

```bash
pip install transformers datasets
```

å¦‚éœ€ GPU æ”¯æ´ï¼Œè«‹ç¢ºèªå·²å®‰è£ CUDA ç›¸é—œå¥—ä»¶ã€‚

---
## 4ï¸âƒ£ æ¨¡å‹æœå°‹èˆ‡è¼‰å…¥

### 4.1 åœ¨ Hugging Face ç¶²ç«™æœå°‹æ¨¡å‹

- é€²å…¥ https://huggingface.co/models

- æ ¹æ“šä»»å‹™é¡å‹ï¼ˆtext-classificationã€text-generation ç­‰ï¼‰èˆ‡èªè¨€ç¯©é¸

### 4.2 è¼‰å…¥æ¨¡å‹èˆ‡ tokenizer

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)
```

---
## 5ï¸âƒ£ ç¯„ä¾‹ï¼šæ–‡å­—åˆ†é¡æ¨è«–

```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_name = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

inputs = tokenizer("I love using Hugging Face!", return_tensors="pt")
outputs = model(**inputs)
predictions = torch.softmax(outputs.logits, dim=-1)
print(predictions)
```

---
## 6ï¸âƒ£ ä½¿ç”¨ Pipeline å¿«é€Ÿå®Œæˆä»»å‹™

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
result = classifier("Hugging Face æ˜¯ä¸€å€‹å¾ˆæ£’çš„å¹³å°ï¼")
print(result)
```

---
## 7ï¸âƒ£ å¾®èª¿ï¼ˆFine-tuningï¼‰åŸºç¤æµç¨‹

- ä½¿ç”¨ `datasets` è¼‰å…¥èˆ‡æº–å‚™è³‡æ–™

- å®šç¾©æ¨¡å‹èˆ‡ tokenizer

- è¨­å®šè¨“ç·´åƒæ•¸èˆ‡ Trainer

- åŸ·è¡Œè¨“ç·´ä¸¦ä¿å­˜æ¨¡å‹

---

## 8ï¸âƒ£ å¤šæ¨¡æ…‹æ¨¡å‹èˆ‡å…¶ä»–æ‡‰ç”¨

- CLIPï¼šæ–‡å­—èˆ‡å½±åƒå…±åŒåµŒå…¥

- Whisperï¼šèªéŸ³è¾¨è­˜

- Vision Transformerï¼ˆViTï¼‰ï¼šå½±åƒåˆ†é¡

---

## ğŸ”— å»¶ä¼¸é–±è®€

- [Hugging Face å®˜æ–¹æ–‡ä»¶](https://huggingface.co/docs)
- [Transformers GitHub](https://github.com/huggingface/transformers)
- [[014.9-AIæ‡‰ç”¨-å¤šæ¨¡æ…‹ AI æ‡‰ç”¨ï¼ˆæ–‡å­—+åœ–ç‰‡+èªéŸ³ï¼‰]]
- [[014.19-AIæ‡‰ç”¨-æ¨¡å‹éƒ¨ç½²èˆ‡å®¹å™¨åŒ– (Docker)]]